{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou8e20tzDq-p",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 8: CNNs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPJ2o5SWDq-u",
    "tags": [
     "text"
    ]
   },
   "source": [
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you, as a student, to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names and more.\n",
    "\n",
    "Within the notebook, we provide detailed instruction which you should follow in order to maximise your final grade.\n",
    "\n",
    "**Name your notebook properly**, follow the pattern in the template name:\n",
    "\n",
    "**Assignment_N_NameSurname_matrnumber**\n",
    "<ol>\n",
    "    <li>N - number of assignment</li>\n",
    "    <li>NameSurname - your full name where every part of the name starts with a capital letter, no spaces</li>\n",
    "    <li>matrnumber - you student number on ID card (without k, potenitially with a leading zero)</li>\n",
    "</ol>\n",
    "\n",
    "Don't add any cells but use the ones provided by us. You may notice that most cells are tagged such that the unittest routine can recognise them.\n",
    "\n",
    "We highly recommend you to develop your code within the provided cells. You can implement helper functions where needed unless you put them in the same cell they are actually called. Always make sure that implemented functions have the correct output and given variables contain the correct data type.\n",
    "\n",
    "**Note:** Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unitest they won't be available either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l0Io1_2Dq-v",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Explicit Computation of CNNs</h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofipfzlXDq-w",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1.1: Convolution and Stride</h2>\n",
    "\n",
    "In this task, you should do some computuations for CNNs explicitly to gain further understanding how the corresponding operations work. \n",
    "\n",
    "<b>Your are not allowed to use any other modules than numpy for all the problems in Task 1.</b>\n",
    "\n",
    "Assume you are given the following input image, represented as $\\mathbf{x}=\\begin{pmatrix} 1 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 1 & 1 & 0 \\\\ 1 & 1 & 0 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 0 & 0 & 1  \\end{pmatrix}$.\n",
    "* Consider the filter $\\mathbf{W}=\\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 2 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}$ and compute $\\mathbf{s}=\\mathbf{W} * \\mathbf{x}$. To do this, implement the function `compute_convolution` which computes the output.\n",
    "* Now add the functionality of using the stride parameter to the function and test it for $S=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFiR4YBCDq-w",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">1.1 Code (20 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "Imports_1"
    ]
   },
   "outputs": [],
   "source": [
    "# only numpy is allowed in Task 1 !\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      "\n",
      "1 0 1 1 0 0 \n",
      "0 1 1 0 0 0 \n",
      "0 1 0 1 1 0 \n",
      "1 1 0 1 1 1 \n",
      "0 1 1 1 0 0 \n",
      "1 1 1 0 0 1 \n",
      "Shape: (6, 6)\n",
      "\n",
      "\n",
      "Filter:\n",
      "\n",
      "1 1 1 \n",
      "0 2 0 \n",
      "1 1 1 \n",
      "Shape: (3, 3)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just run this cell to see your inputs\n",
    "def print_array(name: str, arr:np.ndarray):\n",
    "    \"\"\"Nicely prints your arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        name (title) of printed array\n",
    "    arr : np.ndarray\n",
    "        input array\n",
    "    returns: nothing, just prints name, array and its shape.\n",
    "    \"\"\"   \n",
    "    if type(arr) == bool:\n",
    "        print(\"False\")\n",
    "        print(\"\\nYou may have used invalid input parameters...\")\n",
    "        return\n",
    "    print(f\"{name}:\\n\")\n",
    "    for row in arr:\n",
    "        for element in row:\n",
    "            if np.isclose(element,int(element)):\n",
    "                print(f\"{element:.0f}\", sep= ' ',end=' ')\n",
    "            else: \n",
    "                print(f\"{element:.2f}\", sep= ' ', end=' ')\n",
    "        print()\n",
    "    print(f\"Shape: {arr.shape}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "image = np.array([[1,0,1,1,0,0],[0,1,1,0,0,0],[0,1,0,1,1,0],[1,1,0,1,1,1],[0,1,1,1,0,0],[1,1,1,0,0,1]])\n",
    "filter_ = np.array([[1,1,1],[0,2,0],[1,1,1]])\n",
    "\n",
    "print_array(\"Image\",image)\n",
    "print_array(\"Filter\",filter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "convolution"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_convolution(image:np.ndarray,filter_:np.ndarray,stride:int) -> np.ndarray:\n",
    "    \"\"\"Function that computes the convolution of an image array with a given filter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    filter_ : np.ndarray\n",
    "        Convolution filter\n",
    "    stride : int\n",
    "        Stride parameter for convolution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Convoluted image\n",
    "    \"\"\"\n",
    "    # Check if the dimensions are compatible\n",
    "    if (image.shape[0] - filter_.shape[0]) % stride != 0 or (image.shape[1] - filter_.shape[1]) % stride != 0:\n",
    "        raise ValueError(\"Invalid dimensions for the given stride.\")\n",
    "\n",
    "    # Get the dimensions of the output\n",
    "    output_shape = ((image.shape[0] - filter_.shape[0]) // stride) + 1, ((image.shape[1] - filter_.shape[1]) // stride) + 1\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    convolved = np.zeros(output_shape)\n",
    "\n",
    "    # Perform the convolution operation\n",
    "    for i in range(0, image.shape[0] - filter_.shape[0] + 1, stride):\n",
    "        for j in range(0, image.shape[1] - filter_.shape[1] + 1, stride):\n",
    "            convolved[i // stride, j // stride] = np.sum(image[i:i + filter_.shape[0], j:j + filter_.shape[1]] * filter_)\n",
    "\n",
    "    return convolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "test_conv"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convoluted image with stride=1:\n",
      "\n",
      "5 6 4 3 \n",
      "6 4 5 5 \n",
      "5 5 6 5 \n",
      "7 6 5 4 \n",
      "Shape: (4, 4)\n",
      "\n",
      "\n",
      "Convoluted image with stride=3:\n",
      "\n",
      "5 3 \n",
      "7 4 \n",
      "Shape: (2, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stride_1=compute_convolution(image,filter_,1)\n",
    "assert stride_1.shape == (4,4),(\"Expected shape for stride=1: 4x4\")\n",
    "print_array(\"Convoluted image with stride=1\",stride_1)\n",
    "stride_3=compute_convolution(image,filter_,3)\n",
    "assert stride_3.shape == (2,2),(\"Expected shape for stride=1: 4x4\")\n",
    "print_array(\"Convoluted image with stride=3\",stride_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1.2: Pooling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "After convolution layers, you usually find pooling layers in CNNs to reduce the input size for further layers. To this end, we ask you to implement your own pooling function as well.\n",
    "\n",
    "* Implement the function `pooling(image:np.array,pooling_size:int,pooling_type:str, n_max=1)` which applies one of the following 3 pooling methods, given as the input string parameter `pooling_type`:\n",
    "    1. `\"max-pooling\"` applies max-pooling - using the maximum of all values in the pooling window.\n",
    "    2. `\"mean-pooling\"` applies mean-pooling - using the mean of all values in the pooling window.\n",
    "    3. `\"n-max-pooling\"` applies n-max-pooling for a given optional parameter `n_max:int` - using the mean of the \"n\" maximum values in the\n",
    "    pooling window (defaults to max-pooling). Refer to the lecture material for more elaborate definitions.\n",
    "    4. Raise a ValueError if `pooling_type` is not one of those 3 or `n_max` is larger than the maximum of values in the window in n-max pooling.\n",
    "\n",
    "\n",
    "**Hint:** You can reuse big parts of the previous task for this. Assume that stride=pooling_size for the pooling operation e.g. pooling with non-overlapping windows (like shown in the lecture slides of Unit 7 p.15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">1.2 Code (20 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "pooling"
    ]
   },
   "outputs": [],
   "source": [
    "def pooling(image:np.ndarray,pooling_size:int,pooling_type:str,n_max=1):\n",
    "    \"\"\"Function that applies desired pooling-type on image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    pooling_size : int\n",
    "        Size of pooling window (pooling_size X pooling_size)\n",
    "    pooling_type : str\n",
    "        type of pooling that should be applied, choose from \"max-pooling\",\"mean-pooling\",\"n-max-pooling\"\n",
    "    n_max : int, optional\n",
    "        parameter for n-max-pooling, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Pooled image array\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the desired pooling type does not exist.\n",
    "    ValueError\n",
    "        If pooling_type=\"n_max\" and n_max is larger than there are elements in the pooling window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the pooling type is valid\n",
    "    if pooling_type not in [\"max-pooling\", \"mean-pooling\", \"n-max-pooling\"]:\n",
    "        raise ValueError(\"Invalid pooling type. Choose from 'max-pooling', 'mean-pooling', 'n-max-pooling'.\")\n",
    "\n",
    "    # Check if n_max is valid for n-max-pooling\n",
    "    if pooling_type == \"n-max-pooling\" and n_max > pooling_size**2:\n",
    "        raise ValueError(\"n_max cannot be larger than the number of elements in the pooling window.\")\n",
    "\n",
    "    # Get the dimensions of the output\n",
    "    output_shape = (image.shape[0] // pooling_size, image.shape[1] // pooling_size)\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    pooled = np.zeros(output_shape)\n",
    "\n",
    "    # Perform the pooling operation\n",
    "    for i in range(0, image.shape[0], pooling_size):\n",
    "        for j in range(0, image.shape[1], pooling_size):\n",
    "            window = image[i:i + pooling_size, j:j + pooling_size]\n",
    "\n",
    "            if pooling_type == \"max-pooling\":\n",
    "                pooled[i // pooling_size, j // pooling_size] = np.max(window)\n",
    "            elif pooling_type == \"mean-pooling\":\n",
    "                pooled[i // pooling_size, j // pooling_size] = np.mean(window)\n",
    "            elif pooling_type == \"n-max-pooling\":\n",
    "                pooled[i // pooling_size, j // pooling_size] = np.mean(np.sort(window.flatten())[-n_max:])\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "test_pooling"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max-pooling with window size 2::\n",
      "\n",
      "6 5 \n",
      "7 6 \n",
      "Shape: (2, 2)\n",
      "\n",
      "\n",
      "Mean-pooling with window size 2::\n",
      "\n",
      "5.25 4.25 \n",
      "5.75 5 \n",
      "Shape: (2, 2)\n",
      "\n",
      "\n",
      "N-max-pooling with window size 2 and n_max=3::\n",
      "\n",
      "5.67 4.67 \n",
      "6 5.33 \n",
      "Shape: (2, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect your pooled images - shapes should be 2 X 2 for all of them\n",
    "print_array(\"Max-pooling with window size 2:\",pooling(stride_1,2,\"max-pooling\"))\n",
    "print_array(\"Mean-pooling with window size 2:\",pooling(stride_1,2,\"mean-pooling\"))\n",
    "print_array(\"N-max-pooling with window size 2 and n_max=3:\",pooling(stride_1,2,\"n-max-pooling\",3))\n",
    "\n",
    "# test your pooled images\n",
    "## for n_max = 1, n-max-pooling should == max-pooling, \n",
    "## for n_max = pooling_size**2, n-max-pooling should == mean-pooling\n",
    "assert np.allclose(pooling(image,2,\"max-pooling\"),pooling(image,2,\"n-max-pooling\",1)),(\"For n_max = 1, n-max-pooling should == max-pooling!\")\n",
    "assert np.allclose(pooling(image,2,\"mean-pooling\"),pooling(image,2,\"n-max-pooling\",2**2)),(\"For n_max = pooling_size**2, n-max-pooling should == mean-pooling!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1.3: Padding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "Right now, our implementation for the convolution will decrease the image size in any case, but often one wants to end up with a specific dimension in the end. Therefore we need to manipulate the given image in order to be able to apply the convolution in a way that delivers the desired output.\n",
    "\n",
    "* Implement the function `compute_padding_size()` which calculates the needed padding size given the original size, kernel (=filter) size and a stride parameter to end up with some desired size of the feature map in the end. You can assume that height = width for all entities. Keep in mind that only a non-negative integer solution will make sense in this case, so if the result is not an integer or smaller than 0, return \"False\".\n",
    "\n",
    "* Implement the function `padding(image:np.ndarray, pad_size:int, pad_type:str)` which applies one of the two following padding-operations: \n",
    "    1. Zero-padding: `pad_type=\"zero\"` - Image is padded with `pad_size` number of zeros on all four sides.\n",
    "    \n",
    "        Example: $\\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 2 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix} \\rightarrow$ pad-size = 1, pad_type = \"zero\": $\\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 \\\\ 0 & 0 & 2 & 0 & 0\\\\ 0& 1 & 1 & 1 & 0 \\\\ 0& 0 & 0 & 0 & 0\\end{pmatrix}$\n",
    "    \n",
    "    2. Repeat-padding: `pad_type=\"repeat\"` - Also \"Replication-\" or \"Reflection-padding\", Values at the borders of the image are used to pad the image. Use only the outer-most values, in the corners repeat the value in the corner 3 times for each padding layer.   \n",
    "    Example: $\\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 2 & 0 \\\\ 1 & 1 & \\color{red}1 \\end{pmatrix} \\rightarrow$ pad-size = 1, pad_type = \"repeat\": $\\begin{pmatrix} 0 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 2 & 0 & 0\\\\ 1& 1 & 1 & \\color{pink}1 & \\color{red}1 \\\\ 1& 1 & 1 & \\color{red}1 & \\color{red}1\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">1.3 Code (20 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "padding_size"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_padding(input_size:int,feature_size:int,filter_size:int,stride:int): \n",
    "    \"\"\"\n",
    "    Function that computes necessary padding to receive desired image shape after convolution.\n",
    "    Remember that padding is usually done before the convolution, for now ignore possible pooling afterwards.\n",
    "    It is possible that invalid input parameters lead to \"half-integer\" results, or negative values;\n",
    "    if that is the case the function should return False.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        size of input image\n",
    "    feature_size : int\n",
    "        desired size of final output\n",
    "    filter_size : int\n",
    "        filter size of convolution\n",
    "    stride : int\n",
    "        stride of convolution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The required padding size as integer is returned if input parameters are valid (can be 0)\n",
    "    False\n",
    "        Returns False if the result is not an integer or a negative number\n",
    "    \"\"\"\n",
    "    res = None\n",
    "    padding_size = (feature_size - 1) * stride + filter_size - input_size\n",
    "\n",
    "    # Check for invalid cases\n",
    "    if padding_size % 2 != 0 or padding_size < 0:\n",
    "        return False\n",
    "\n",
    "    return padding_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "paddint_size_test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary padding:  4\n",
      "Necessary padding:  0\n"
     ]
    }
   ],
   "source": [
    "# compute padding for 4x4 input, desired output: 6x6, filter size 2x2 and stride = 2: \n",
    "assert compute_padding(4,6,2,2) == 4,(\"Wrong result for compute_padding.\")\n",
    "assert type(compute_padding(4,2,2,2)) == int, (\"Padding of zero should also be possible.\")\n",
    "print(\"Necessary padding: \",compute_padding(4,6,2,2))\n",
    "print(\"Necessary padding: \",compute_padding(4,2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "padding"
    ]
   },
   "outputs": [],
   "source": [
    "def padding(image:np.ndarray, pad_size:int, pad_type:str):\n",
    "    \"\"\"Function that pads an Image with either zero-padding or repeat-padding.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        input image\n",
    "    pad_size : int\n",
    "        how much padding should be applied on either side of the image, should also work with pad_size = 0\n",
    "    pad_type : str\n",
    "        type of padding: \"zero\" or \"repeat\" \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        padded image array\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        Raises ValueError for inputs other than \"zero\" or \"repeat\" for pad_type\n",
    "    ValueError\n",
    "        Raises ValueError if pad_size is smaller than 0.\n",
    "\n",
    "    \"\"\"\n",
    "    padded = None\n",
    "    #Your code goes here ↓↓↓    \n",
    "    # dont forget your input checks \n",
    "    #if ...\n",
    "    #    raise ...\n",
    "    # Input checks\n",
    "    if pad_type not in [\"zero\", \"repeat\"]:\n",
    "        raise ValueError(\"Invalid pad_type. Choose from 'zero' or 'repeat'.\")\n",
    "\n",
    "    if pad_size < 0:\n",
    "        raise ValueError(\"pad_size cannot be smaller than 0.\")\n",
    "\n",
    "    # Pad the image\n",
    "    if pad_type == \"zero\":\n",
    "        padded = np.pad(image, pad_size, mode='constant', constant_values=0)\n",
    "    elif pad_type == \"repeat\":\n",
    "        padded = np.pad(image, pad_size, mode='edge')\n",
    "\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "padding_test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image::\n",
      "\n",
      "0 0 1 1 \n",
      "0 1 1 0 \n",
      "0 1 0 1 \n",
      "1 1 0 1 \n",
      "Shape: (4, 4)\n",
      "\n",
      "\n",
      "Image after zero-padding with padding size 3:\n",
      "\n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 1 1 0 0 0 \n",
      "0 0 0 0 1 1 0 0 0 0 \n",
      "0 0 0 0 1 0 1 0 0 0 \n",
      "0 0 0 1 1 0 1 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 \n",
      "Shape: (10, 10)\n",
      "\n",
      "\n",
      "Image after repeat-padding with padding size 3::\n",
      "\n",
      "0 0 0 0 0 1 1 1 1 1 \n",
      "0 0 0 0 0 1 1 1 1 1 \n",
      "0 0 0 0 0 1 1 1 1 1 \n",
      "0 0 0 0 0 1 1 1 1 1 \n",
      "0 0 0 0 1 1 0 0 0 0 \n",
      "0 0 0 0 1 0 1 1 1 1 \n",
      "1 1 1 1 1 0 1 1 1 1 \n",
      "1 1 1 1 1 0 1 1 1 1 \n",
      "1 1 1 1 1 0 1 1 1 1 \n",
      "1 1 1 1 1 0 1 1 1 1 \n",
      "Shape: (10, 10)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test your implementation\n",
    "image_test = np.array([[0,0,1,1],[0,1,1,0],[0,1,0,1],[1,1,0,1]])\n",
    "filter_test = np.array([[1,1],[0,2]])\n",
    "\n",
    "correct_ = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 1, 1, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 1, 0, 1, 0, 0, 0,],\n",
    "                    [0, 0, 0, 1, 1, 0, 1, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,]])\n",
    "print_array(\"Original Image:\", image_test)\n",
    "assert np.allclose(correct_, padding(image_test,3,\"zero\")),(\"Zero-padding result not correct!\")\n",
    "print_array(\"Image after zero-padding with padding size 3\", padding(image_test,3,'zero'))\n",
    "print_array(\"Image after repeat-padding with padding size 3:\",padding(image_test,3,'repeat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1.4: The Whole Process</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "Now everything should be in place and we can combine the implemented solution into one pipeline.\n",
    "\n",
    "* Implement the function `process_image(...)` which takes an image, output size, filter, a stride parameter and all other necessary inputs for the sub-functions as input and first computes the convolution followed by pooling. \n",
    "Keep in mind, that the image might have to be padded before application of the convolution to get the desired output shape.\n",
    "\n",
    "**Note:** You can still assume that both image and kernel are quadratic (i.e. height = width)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">1.4 Code (15 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "process"
    ]
   },
   "outputs": [],
   "source": [
    "def process_image(image:np.ndarray,\n",
    "                  feature_size:int,\n",
    "                  filter_:np.ndarray,\n",
    "                  stride:int,\n",
    "                  pooling_size:int,\n",
    "                  pooling_type:str,\n",
    "                  padding_type:str,\n",
    "                  compute_padding_size_function:callable,\n",
    "                  padding_function:callable,\n",
    "                  compute_convolution_function:callable,\n",
    "                  pooling_function:callable,\n",
    "                  n_max = 1):\n",
    "    \"\"\"Function that processes an image array. It first calculates the necessary padding, if padding is possible (integer) it\n",
    "    performs the padding, followed by convolution and pooling.\n",
    "    Your previously implemented functions will be fed to this function.\n",
    "    Make sure to only use the functions and their respective names provided inside this function!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        input image\n",
    "    feature_size : int\n",
    "        desired shape after convolution\n",
    "    filter_ : np.ndarray\n",
    "        filter (\"kernel\") applied during convolution\n",
    "    stride : int\n",
    "        stride parameter of convolution\n",
    "    pooling_size : int\n",
    "        pooling size\n",
    "    pooling_type : str\n",
    "        type of pooling (mean-, max- or n-max-pooling)\n",
    "    padding_type : str\n",
    "        type of padding (zero or repeat)\n",
    "    compute_padding_size_function : callable\n",
    "        the compute padding size function implemented by you, use is to compute the padding size\n",
    "    padding_function : callable\n",
    "        the padding function implemented by you, use it to pad the input image\n",
    "    compute_convolution_function : callable\n",
    "        the convolution function implemented by you, use it to convolve the (padded) input image\n",
    "    pooling_function : callable\n",
    "        the pooling function implemented by you, use it to apply pooling on your convolved image\n",
    "    n_max: int, optional\n",
    "        n_max parameter for n-max-pooling if applied, defaults to 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Returns processed image if inputs are valid and padding is possible (also if padding is just 0!)\n",
    "        else: returns False\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # Step 1: Compute necessary padding\n",
    "    padding_size = compute_padding_size_function(image.shape[0], feature_size, filter_.shape[0], stride)\n",
    "\n",
    "    # Check if padding is possible (integer) and non-negative\n",
    "    if padding_size is False:\n",
    "        return False\n",
    "\n",
    "    # Step 2: Apply padding to the input image\n",
    "    padded_image = padding_function(image, padding_size, padding_type)\n",
    "\n",
    "    # Step 3: Apply convolution\n",
    "    convolved_image = compute_convolution_function(padded_image, filter_, stride)\n",
    "\n",
    "    # Step 4: Apply pooling\n",
    "    processed_image = pooling_function(convolved_image, pooling_size, pooling_type, n_max)\n",
    "\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your result::\n",
      "\n",
      "0 4 4 \n",
      "2 3 4 \n",
      "4 4 4 \n",
      "Shape: (3, 3)\n",
      "\n",
      "\n",
      "Solution::\n",
      "\n",
      "0 4 4 \n",
      "2 3 4 \n",
      "4 4 4 \n",
      "Shape: (3, 3)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test your implementation\n",
    "correct_5 = np.array([[0, 4, 4,], [2, 3, 4,], [4, 4, 4,]])\n",
    "res = process_image(image_test,6,filter_test,2,2,\"max-pooling\",\"repeat\",compute_padding,padding,compute_convolution,pooling)\n",
    "print_array(\"Your result:\", res)\n",
    "print_array(\"Solution:\", correct_5)\n",
    "assert np.allclose(res,correct_5),(\"Your result does not match the solution!\")\n",
    "assert process_image(image_test,6,filter_test,1,2,\"max-pooling\",\"repeat\",compute_padding,padding,compute_convolution,pooling)==False,(\"Should return False for invalid inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "playground"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed original Image with max-pooling and zero-padding::\n",
      "\n",
      "7 \n",
      "Shape: (1, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here you can play around with the original image and filter\n",
    "print_array(\"Processed original Image with max-pooling and zero-padding:\", process_image(image,2,filter_,3,2,\"max-pooling\",\"repeat\",compute_padding,padding,compute_convolution,pooling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThpuXicXDq-x",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: CNNs vs. the Rest</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKF1LOgqYLSg",
    "tags": [
     "text"
    ]
   },
   "source": [
    "In this task, we will carry out a comparison of several classifiers on different portions of the FashionMNIST data set. From the whole training data we create smaller training sets that have $[0.05,0.10,0.25, 0.50, 1.00]$ times the the size of the total set. Compared to the previous assignment, the data loader is modified a little so that it outputs different formats for the computations with Pytorch, where we use tensors, and sklearn, where we use numpy arrays.\n",
    "We make our comparison based on three metrics:\n",
    "* Accuracy: The standard (but not necessarily best) metric for evaluating the performance of a model in predicting the labels of unseen samples (correct predictions divided by number of samples in test set).\n",
    "* Training Time: For RandomForest and SVM this is the time measured for fitting the model on the training data, for the CNNs it's the time measured from initialization of the model until the end of the last training epoch (sum over all epochs).\n",
    "* Inference Time: Time measured for the model to calculate predictions on the test data, for CNNs it's the mean of the time taken to calculate predictions over all epochs.\n",
    "\n",
    "To save computation time, you may also use the hard-coded experiment results at the end of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6-awJvdDq-x",
    "tags": [
     "imports2"
    ]
   },
   "outputs": [],
   "source": [
    "## To install pytorch for this assignment use:\n",
    "#!conda install pytorch torchvision -c pytorch -y\n",
    "## OR if you use pip\n",
    "#!pip install pytorch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPJI5ZnxDq-z",
    "tags": [
     "setting"
    ]
   },
   "outputs": [],
   "source": [
    "int_classes = int \n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "9IVEE1WYDq-1",
    "outputId": "cb55cb33-b398-4390-ade8-cb4a68b07aa0",
    "scrolled": true,
    "tags": [
     "sampler"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b8c9461fda4a78a876bb0a7cb3cc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b4366e984a47a38eafab3436ac6b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa2abe492904e5d984e14cb5fd2cd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f7def7c27d4c9baebef001caf7c58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### use Pytorch dataloader with a subset of the training data\n",
    "def get_sampler(N_samples):  \n",
    "    mask = list(np.arange(N_samples))  ## list of valid sample_ids\n",
    "    return torch.utils.data.RandomSampler(mask) ## random order\n",
    "\n",
    "def get_data_loader(use_cuda,batch_size=64,train=True,get_all=False,sampler=None):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        #datasets.FashionMNIST(os.path.join('.','..','data')\n",
    "        datasets.FashionMNIST(os.path.join('data')\n",
    "                            ,train=train, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.2859,), (0.3530,))\n",
    "                       ])), shuffle=False, sampler=sampler, batch_size=batch_size,**kwargs)\n",
    "    ## return numpy arrays of the dataset\n",
    "    if get_all: \n",
    "        for _, (train_samples, train_labels) in enumerate(loader):\n",
    "            return train_samples.numpy().reshape(-1,28*28),train_labels.numpy()\n",
    "    ## return loader to provide minibatches\n",
    "    else:\n",
    "        return loader\n",
    "\n",
    "### prepare data for sklearn models\n",
    "train_samples, train_labels = get_data_loader(use_cuda,batch_size=6*10**4,train=True,get_all = True)\n",
    "test_samples, test_labels = get_data_loader(use_cuda,batch_size=10**4,train=False,get_all = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqjz-K2QYLSr",
    "tags": [
     "text"
    ]
   },
   "source": [
    "Now we want to apply different models to our prepared data. We also want to print and store accuracy, training time and inference time, so these should be our results. In the dictionary \"experiments\", we want to store these informations in an array for the different models and different training data sizes. Just run the code in oder to understand better what is meant exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "1RY4eW83Dq-3",
    "outputId": "94cb6f0f-a85d-417e-913f-59812c9509dd",
    "tags": [
     "dictionary"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percentages': array([0.05, 0.1 , 0.25, 0.5 , 1.  ]),\n",
       " 'N_samples': array([ 3000,  6000, 15000, 30000, 60000]),\n",
       " 'RF_100': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'RF_500': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'SVM': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'CNN_simple': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'CNN_wide': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'CNN_deep': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])},\n",
       " 'CNN_wide_max': {'accuracy': array([0., 0., 0., 0., 0.]),\n",
       "  'training_time': array([0., 0., 0., 0., 0.]),\n",
       "  'inference_time': array([0., 0., 0., 0., 0.])}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"RF_100\",\"RF_500\",\"SVM\",\"CNN_simple\",\"CNN_wide\",\"CNN_deep\",\"CNN_wide_max\"]\n",
    "results = [\"accuracy\",\"training_time\",\"inference_time\"]\n",
    "experiments = {}\n",
    "experiments[\"percentages\"] = np.array([0.05,0.10,0.25,0.50,1.00])\n",
    "experiments[\"N_samples\"] = (60000 * experiments[\"percentages\"]).astype(int)\n",
    "\n",
    "for k in models:\n",
    "    experiments[k] = {}\n",
    "    for l in results:\n",
    "        experiments[k][l] = np.zeros([len(experiments[\"N_samples\"])])\n",
    "\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIz9FAWwYLSu",
    "tags": [
     "text"
    ]
   },
   "source": [
    "Next, we want to apply this routine to the sklearn models first (i.e RFs and SVMs) for the different sizes of the data set. \n",
    "We implemented the routine for Random Forest with 100 estimators, with 500 estimators and for SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "c_XUfWb1Dq-9",
    "outputId": "c596d7a8-cb2e-4bf9-c734-7dddb943c500",
    "tags": [
     "exec"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset consists of 3000 samples\n",
      "training took 13.31 seconds\n",
      "inference took 0.56 seconds\n",
      "test accuracy: 81.95 percent\n",
      "Subset consists of 6000 samples\n",
      "training took 23.61 seconds\n",
      "inference took 0.56 seconds\n",
      "test accuracy: 83.16 percent\n",
      "Subset consists of 15000 samples\n",
      "training took 49.10 seconds\n",
      "inference took 0.55 seconds\n",
      "test accuracy: 83.97 percent\n",
      "Subset consists of 30000 samples\n",
      "training took 89.03 seconds\n",
      "inference took 0.56 seconds\n",
      "test accuracy: 84.20 percent\n",
      "Subset consists of 60000 samples\n",
      "training took 165.24 seconds\n",
      "inference took 0.56 seconds\n",
      "test accuracy: 84.22 percent\n"
     ]
    }
   ],
   "source": [
    "### code for experiments['RF_100'], just execute\n",
    "for i,n_samples in enumerate(experiments[\"N_samples\"]):\n",
    "    print(\"Subset consists of {} samples\".format(n_samples))\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=seed)\n",
    "    \n",
    "    start_train= time.time()\n",
    "    model.fit(train_samples[:n_samples],train_labels[:n_samples])\n",
    "    end_train = time.time()\n",
    "    train_time = np.round(end_train-start_train,decimals=2)\n",
    "    experiments[\"RF_100\"][\"training_time\"][i] = train_time \n",
    "    print(\"training took {:.2f} seconds\".format(train_time))\n",
    "    \n",
    "    start_infer= time.time()\n",
    "    pred = model.predict(test_samples)\n",
    "    end_infer = time.time()\n",
    "    infer_time = np.round(end_infer-start_infer,decimals=2)\n",
    "    experiments[\"RF_100\"][\"inference_time\"][i] = infer_time\n",
    "    print(\"inference took {:.2f} seconds\".format(infer_time))\n",
    "    \n",
    "    accuracy = np.round(sum((pred-test_labels)==0)/len(test_labels)*100,\n",
    "                        decimals=2)\n",
    "    print(\"test accuracy: {:.2f} percent\".format(accuracy))\n",
    "    experiments[\"RF_100\"][\"accuracy\"][i] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "c_XUfWb1Dq-9",
    "outputId": "c596d7a8-cb2e-4bf9-c734-7dddb943c500",
    "tags": [
     "exec"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset consists of 3000 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25980\\1739254501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mend_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_train\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \"\"\"\n\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         super()._fit(\n\u001b[0m\u001b[0;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    441\u001b[0m             )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### code for experiments['RF_500'], just execute\n",
    "for i,n_samples in enumerate(experiments[\"N_samples\"]):\n",
    "    print(\"Subset consists of {} samples\".format(n_samples))\n",
    "    model = RandomForestClassifier(n_estimators=500,max_depth=10,random_state=seed)\n",
    "    \n",
    "    start_train= time.time()\n",
    "    model.fit(train_samples[:n_samples],train_labels[:n_samples])\n",
    "    end_train = time.time()\n",
    "    train_time = np.round(end_train-start_train,decimals=2)\n",
    "    experiments[\"RF_500\"][\"training_time\"][i] = train_time \n",
    "    print(\"training took {:.2f} seconds\".format(train_time))\n",
    "    \n",
    "    start_infer= time.time()\n",
    "    pred = model.predict(test_samples)\n",
    "    end_infer = time.time()\n",
    "    infer_time = np.round(end_infer-start_infer,decimals=2)\n",
    "    experiments[\"RF_500\"][\"inference_time\"][i] = infer_time\n",
    "    print(\"inference took {:.2f} seconds\".format(infer_time))\n",
    "    \n",
    "    accuracy = np.round(sum((pred-test_labels)==0)/len(test_labels)*100,\n",
    "                        decimals=2)\n",
    "    print(\"test accuracy: {:.2f} percent\".format(accuracy))\n",
    "    experiments[\"RF_500\"][\"accuracy\"][i] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "xhoMss4xYLS0",
    "outputId": "bcc2cf4c-94c0-4ea9-9213-8bece5998771",
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "### code for experiments['SVM'], just execute\n",
    "for i,n_samples in enumerate(experiments[\"N_samples\"]):\n",
    "    print(\"Subset consists of {} samples\".format(n_samples))\n",
    "    model = SVC(gamma=0.1,kernel='poly',degree=5,random_state=seed)\n",
    "    \n",
    "    start_train= time.time()\n",
    "    model.fit(train_samples[:n_samples],train_labels[:n_samples])\n",
    "    end_train = time.time()\n",
    "    train_time = np.round(end_train-start_train,decimals=2)\n",
    "    experiments[\"SVM\"][\"training_time\"][i] = train_time \n",
    "    print(\"training took {:.2f} seconds\".format(train_time))\n",
    "    \n",
    "    start_infer= time.time()\n",
    "    pred = model.predict(test_samples)\n",
    "    end_infer = time.time()\n",
    "    infer_time = np.round(end_infer-start_infer,decimals=2)\n",
    "    experiments[\"SVM\"][\"inference_time\"][i] = infer_time\n",
    "    print(\"inference took {:.2f} seconds\".format(infer_time))\n",
    "    \n",
    "    accuracy = np.round(sum((pred-test_labels)==0)/len(test_labels)*100,\n",
    "                        decimals=2)\n",
    "    print(\"test accuracy: {:.2f} percent\".format(accuracy))\n",
    "    experiments[\"SVM\"][\"accuracy\"][i] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7WOSvBaDq_A",
    "tags": [
     "text"
    ]
   },
   "source": [
    "Now we want to run similar experiments with four different CNN models. Feel free to experiment with the networks. \n",
    "We first provide the training and test routine for the CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2zYlWcKDq_A",
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "#nothing to do here, just execute\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train() \n",
    "    correct=0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.shape[0]\n",
    "    print('Epoch {} \\nTraining Accuracy: {}/{} ({:.2f}%)'.format(epoch,\n",
    "    correct, total, 100*correct/total \n",
    "    )) \n",
    "    \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = np.round(100. * correct / len(test_loader.dataset),decimals=2)\n",
    "    print('Test Accuracy: {}/{} ({:.2f}%)'.format(correct, len(test_loader.dataset), accuracy)) \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw6d1K-DYLS-",
    "tags": [
     "text"
    ]
   },
   "source": [
    "Here we provide a routine that creates CNN models. It takes as inputs the hyper-parameters of the CNNs. It's not necessary to fully understand this routine at this stage, as this will be a main topic in further courses (e.g. Deep Learning 1&2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NRK1hMDDq_C",
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "#nothing to do here, just execute\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,use_batch_norm=True,n_blocks=3,n_layers=3,channels = 32, multiply_channels=2, global_max = True):\n",
    "        super(Net, self).__init__(),\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.channels = channels\n",
    "        self.multiply_channels = multiply_channels\n",
    "        self.global_max = global_max\n",
    "\n",
    "        ## feature extraction CNN => linear layer (N_cannels to N_classes) => softmax\n",
    "        self.cnn_module = self.build_model() \n",
    "        self.fc_module = nn.Sequential(\n",
    "            nn.Linear(channels*multiply_channels**(n_blocks-1), 10)) \n",
    "        \n",
    "    def build_model(self):\n",
    "        channels_per_layer = [1,self.channels]\n",
    "        for i in range(1,self.n_blocks):\n",
    "            channels_per_layer.append(self.channels*self.multiply_channels**i)   \n",
    "        \n",
    "        components = []\n",
    "        for i in range(self.n_blocks):\n",
    "            for j in range(self.n_layers):\n",
    "                if j== 0:\n",
    "                    cur_dims = [channels_per_layer[i],channels_per_layer[i+1]]   ## first layer of the block\n",
    "                else: \n",
    "                    cur_dims = [channels_per_layer[i+1],channels_per_layer[i+1]]\n",
    "\n",
    "                if self.use_batch_norm:        ## no bias needed\n",
    "                    components.append(\n",
    "                        nn.Sequential(nn.Conv2d(cur_dims[0], cur_dims[1], kernel_size = 3,padding=1,bias=False),\n",
    "                                     nn.BatchNorm2d(cur_dims[1], momentum=0.1),\n",
    "                                     nn.ReLU()\n",
    "                                     )\n",
    "                    )\n",
    "                else:                          \n",
    "                    components.append(\n",
    "                        nn.Sequential(nn.Conv2d(cur_dims[0], cur_dims[1], kernel_size = 3,padding=1),\n",
    "                                     nn.ReLU()\n",
    "                                     )\n",
    "                    )\n",
    "            if i == self.n_blocks-1:\n",
    "                if self.global_max:\n",
    "                    components.append(nn.Sequential(nn.AdaptiveMaxPool2d(1)))    ## finish with a global max pooling layer\n",
    "                else:\n",
    "                    components.append(nn.Sequential(nn.AdaptiveAvgPool2d(1)))    ## finish with a global average pooling layer\n",
    "            else: \n",
    "                components.append(nn.Sequential(nn.MaxPool2d(2, stride = 2)))    ## downsampling via max_pooling of stride 2\n",
    "        return nn.Sequential(*components)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        x = self.cnn_module(x)    \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_module(x)     \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9gtHy6_YLTE",
    "tags": [
     "text"
    ]
   },
   "source": [
    "Now let us run the different CNN models for the different sizes of the data sets.\n",
    "You should run the experiments with the following four CNN models:\n",
    "- a simple CNN with average pooling, called \"CNN_simple\"\n",
    "- a wide CNN with average pooling, called \"CNN_wide\"\n",
    "- a deep CNN with average pooling, called \"CNN_deep\"\n",
    "- a wide CNN with max pooling, called \"CNN_wide_max\"\n",
    "\n",
    "This is done now in a similar fashion as for the sklearn methods. Again accuracy, training time, and test time are stored in the experiments dictionary for the different sizes of the data sets. The number of trainable parameters for each of the four different CNNs is printed in the begining of the training loop of the corresponding model. This should allow you to get a rough idea of the complexities of these models.\n",
    "\n",
    "<b>This can take up to 1h, depending on your hardware, maybe you go grab a coffee in the meantime..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kAHUquqsDq_I",
    "outputId": "0bffa978-da10-4d0b-e39b-7d9974c23f50",
    "scrolled": true,
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "## nothing to do here, just execute\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "for model_tag in [\"CNN_simple\",\"CNN_wide\",\"CNN_deep\",\"CNN_wide_max\"]:\n",
    "    for i,n_samples in enumerate(experiments[\"N_samples\"]):\n",
    "        if  model_tag == \"CNN_simple\":\n",
    "            ## a simple 'CNN with 3 layers with 16 channels each\n",
    "            model = Net(use_batch_norm=True,n_blocks=3,n_layers=1,channels = 16, multiply_channels=1,global_max=False).to(device)  ## simple\n",
    "        elif model_tag == \"CNN_wide\":\n",
    "            ## a wider version with 3 layers with 16,32 and 64 channels\n",
    "            model = Net(use_batch_norm=True,n_blocks=3,n_layers=1,channels = 32, multiply_channels=2,global_max=False).to(device)  ## wide\n",
    "        elif model_tag == \"CNN_deep\":\n",
    "            ## a deeper version with 9 layers with 16 channels each\n",
    "            model = Net(use_batch_norm=True,n_blocks=3,n_layers=3,channels = 16, multiply_channels=1,global_max=False).to(device)  ## deep\n",
    "        elif model_tag == \"CNN_wide_max\":\n",
    "            ## a wider version with 3 layers with 16, 32 and 64 channels, global maximum pooling\n",
    "            model = Net(use_batch_norm=True,n_blocks=3,n_layers=1,channels = 32, multiply_channels=2,global_max=True).to(device)  ## wide       \n",
    "\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        sampler = get_sampler(experiments[\"N_samples\"][i])\n",
    "        train_loader = get_data_loader(use_cuda,64,train=True,sampler=sampler)\n",
    "        test_loader = get_data_loader(use_cuda,128,train=False)\n",
    "\n",
    "        if i == 0:\n",
    "            print(model)\n",
    "            print(\"\\nThe model has {} parameters\\n\".format(sum(p.numel() for p in model.parameters())))\n",
    "            \n",
    "        print(\"Subset consists of {} samples\".format(n_samples))\n",
    "        epoch_times = []\n",
    "        infer_times = []\n",
    "        accuracies = []\n",
    "        for epoch in range(1,max_epochs+1):\n",
    "            start_epoch=time.time()\n",
    "            train(model, device, train_loader, optimizer, epoch)\n",
    "            end_epoch=time.time()\n",
    "            epoch_time = np.round(end_epoch-start_epoch,decimals=2)\n",
    "            epoch_times.append(epoch_time)\n",
    "            \n",
    "            print(\"epoch took {:.2f} seconds\".format(epoch_time))\n",
    "            start_infer = time.time()\n",
    "            accuracies.append(test(model, device, test_loader))\n",
    "            end_infer=time.time()\n",
    "            infer_time = np.round(end_infer-start_infer,decimals=2)\n",
    "            infer_times.append(infer_time)\n",
    "            print(\"inference took {:.2f} seconds\".format(infer_time))\n",
    "        \n",
    "        print(\"finished \" + model_tag + \" with {}\".format(n_samples))\n",
    "        experiments[model_tag][\"accuracy\"][i] = np.round(np.mean(np.array(accuracies[-3:])),decimals=2)  #average over final 3 epochs\n",
    "        experiments[model_tag][\"training_time\"][i] = np.round(np.sum(np.array(epoch_times)),decimals=2)  # sum over all epochs\n",
    "        experiments[model_tag][\"inference_time\"][i] = np.round(np.mean(np.array(infer_times)),decimals=2)  #mean inference time\n",
    "        for k in experiments[model_tag].keys():\n",
    "            print(k, experiments[model_tag][k])\n",
    "        print()\n",
    "\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "# if you dont want to wait, or dont trust your results, you can use these hard-coded values\n",
    "# just set the variable to True\n",
    "i_cant_wait = False\n",
    "\n",
    "if i_cant_wait:\n",
    "    experiments ={'percentages': np.array([0.05, 0.1 , 0.25, 0.5 , 1.  ]),\n",
    " 'N_samples': np.array([ 3000,  6000, 15000, 30000, 60000]),\n",
    " 'RF_100': {'accuracy': np.array([81.95, 83.16, 83.97, 84.2 , 84.22]),\n",
    "  'training_time': np.array([ 3.88,  7.04, 14.97, 26.81, 48.92]),\n",
    "  'inference_time': np.array([0.32, 0.35, 0.35, 0.35, 0.36])},\n",
    " 'RF_500': {'accuracy': np.array([82.02, 83.38, 84.17, 84.33, 84.54]),\n",
    "  'training_time': np.array([ 19.58,  35.38,  75.22, 136.4 , 248.76]),\n",
    "  'inference_time': np.array([1.63, 1.69, 1.74, 1.81, 1.82])},\n",
    " 'SVM': {'accuracy': np.array([81.34, 83.63, 86.07, 87.9 , 89.21]),\n",
    "  'training_time': np.array([  0.67,   2.05,  25.23,  87.59, 285.24]),\n",
    "  'inference_time': np.array([ 2.1 ,  7.13, 20.22, 36.  , 63.75])},\n",
    " 'CNN_simple': {'accuracy': np.array([61.28, 70.95, 75.11, 77.88, 81.79]),\n",
    "  'training_time': np.array([ 4.75,  9.35, 23.41, 47.32, 94.26]),\n",
    "  'inference_time': np.array([2.11, 2.08, 2.09, 2.1 , 2.1 ])},\n",
    " 'CNN_wide': {'accuracy': np.array([74.97, 73.27, 81.03, 80.41, 86.37]),\n",
    "  'training_time': np.array([  8.74,  17.56,  43.59,  87.59, 176.37]),\n",
    "  'inference_time': np.array([3.13, 3.13, 3.1 , 3.11, 3.11])},\n",
    " 'CNN_deep': {'accuracy': np.array([68.77, 77.11, 83.79, 85.49, 90.01]),\n",
    "  'training_time': np.array([  8.82,  17.75,  43.92,  87.58, 176.93]),\n",
    "  'inference_time': np.array([2.66, 2.66, 2.68, 2.66, 2.68])},\n",
    " 'CNN_wide_max': {'accuracy': np.array([83.38, 86.06, 87.44, 89.61, 91.11]),\n",
    "  'training_time': np.array([  9.16,  18.65,  45.46,  90.35, 182.02]),\n",
    "  'inference_time': np.array([3.28, 3.26, 3.27, 3.24, 3.18])}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lA-D6NHrDq_K",
    "tags": [
     "text"
    ]
   },
   "source": [
    "As a final task, create the following three plots:\n",
    "- Accuracies against fraction of the dataset used for training\n",
    "- Inference times against fraction of the dataset used for training\n",
    "- Training times against fraction of the dataset used for training\n",
    "\n",
    "Compare all seven models in each of these three plots and don't forget to label the plots appropriately! Then answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "_Y7As1-FYLTJ",
    "outputId": "11185453-bcf3-4540-cfa0-4e8457282aa9",
    "tags": [
     "text"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\"> 2.1 Code & Questions (5+20 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "colab_type": "code",
    "id": "yHLowqaYYLTL",
    "outputId": "a1fe376c-e688-4f40-ce39-eb5a24daabc3",
    "scrolled": true,
    "tags": [
     "plot"
    ]
   },
   "outputs": [],
   "source": [
    "def plot(experiments:dict, models:list):\n",
    "    \"\"\"Function to plot Accuracies, Inference Times and Training times of the models in \"experiments\" \n",
    "    against fractions of data used for training.\n",
    "    Plot all 3 plots on the same figure as subplots and make sure you return the right figure!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiments : dict\n",
    "        Experiment results for all models in a dictionary.\n",
    "    models : list \n",
    "        list of model names, also the keys of the dictionary to access the measured data\n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure,\n",
    "        the Figure object, filled with your 3 plots.\n",
    "    \"\"\"\n",
    "    #get fractions used:\n",
    "    percentages = experiments['percentages']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plotting accuracies\n",
    "    for model in models:\n",
    "        axes[0].plot(percentages, experiments[model]['accuracy'], label=model)\n",
    "    axes[0].set_title('Accuracies vs Fraction of Dataset')\n",
    "    axes[0].set_xlabel('Fraction of Dataset Used')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plotting inference times\n",
    "    for model in models:\n",
    "        axes[1].plot(percentages, experiments[model]['inference_time'], label=model)\n",
    "    axes[1].set_title('Inference Times vs Fraction of Dataset')\n",
    "    axes[1].set_xlabel('Fraction of Dataset Used')\n",
    "    axes[1].set_ylabel('Inference Time (seconds)')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Plotting training times\n",
    "    for model in models:\n",
    "        axes[2].plot(percentages, experiments[model]['training_time'], label=model)\n",
    "    axes[2].set_title('Training Times vs Fraction of Dataset')\n",
    "    axes[2].set_xlabel('Fraction of Dataset Used')\n",
    "    axes[2].set_ylabel('Training Time (seconds)')\n",
    "    axes[2].legend()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "plot_test"
    ]
   },
   "outputs": [],
   "source": [
    "# view your plot\n",
    "fig = plot(experiments,models)\n",
    "assert type(fig) == plt.Figure,(\"Plot function does not return Matplotlib Figure!\")\n",
    "assert len(fig.get_axes()) == 3,(\"Figure should consist of 3 subplots\")\n",
    "assert len(fig.get_axes()[0].get_xlabel()) != 0, (\"Label your axes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "Which statements about the plots are correct?\n",
    "\n",
    "a_) Inference time follows a similar pattern for all but one classifier.<br>\n",
    "b_) All models exhibit a linear relation between training time and data set size.<br>\n",
    "c_) CNN_simple has the lowest accuracy for all dataset sizes.<br>\n",
    "d_) The worst accuracy for a model on any dataset is lower than 40%. <br>\n",
    "e_) The model with the shortest training time for a dataset fraction of 0.25 is a random forest.  <br>\n",
    "f_) All classifiers have an accuracy higher than 80% when being trained on the full dataset.<br>\n",
    "g_) Inference and training times indicate that SVMs are not performing well on large data sets compared to the CNN models.<br>\n",
    "h_) A comparison of the performance of any two or more different algorithms is fair as long as they are compared on the same problem, i.e. the same training and test data.<br>\n",
    "i_) A comparison of the performance of any two or more different algorithms should take into account the number of trainable model parameters and the training and inference times.<br>\n",
    "j_) Comparing results for CNN_wide(_max) and CNN_deep, all three are very close in accuracy, thus the wide networks would be the preferred option, as wide CNN's generally train much faster than deep ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "Questions"
    ]
   },
   "outputs": [],
   "source": [
    "# only use True and False as answers!\n",
    "# examples for you:\n",
    "\n",
    "statement_is_true = True\n",
    "statement_is_false = False\n",
    "\n",
    "# your answers go here ↓↓↓\n",
    "\n",
    "a_=True \n",
    "b_=False\n",
    "c_=True\n",
    "d_=False\n",
    "e_=True \n",
    "f_=True\n",
    "g_=True\n",
    "h_=True\n",
    "i_=False\n",
    "j_=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "The take home messages of this task are:\n",
    "- The model choice depends on the size and complexity of the data set\n",
    "- The model choice is constrained by the available computing and time resources\n",
    "- Neural networks are often a good choice for large data sets, e.g. CNNs on computer vision tasks\n",
    "- For smaller problems alternatives like SVM or Random Forests can be more efficient\n",
    "- There is no algorithm that guarantees superior performance on every problem (aka \"No free lunch\"-Theorem)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_8_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
